{"cells":[{"metadata":{},"cell_type":"markdown","source":"# L4: Word embeddings"},{"metadata":{},"cell_type":"markdown","source":"In this lab you will explore word embeddings. A **word embedding** is a mapping of words to points in a vector space such that nearby words (points) are similar in terms of their distributional properties. You will use word embedding to find similar words, and evaluate their usefulness in an inference task.\n\nYou will use the word vectors that come with [spaCy](http://spacy.io). Note that you will need the &lsquo;large&rsquo; English language model; the &lsquo;small&rsquo; model that you used in previous labs does not include proper word vectors."},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\n\nnlp = spacy.load(\"en_core_web_lg\")","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Every word in the model&rsquo;s vocabulary comes with a 300-dimensional vector, represented as a NumPy array. The following code cell shows how to access the vector for the word *cheese*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp.vocab[\"cheese\"].vector","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"array([-5.5252e-01,  1.8894e-01,  6.8737e-01, -1.9789e-01,  7.0575e-02,\n        1.0075e+00,  5.1789e-02, -1.5603e-01,  3.1941e-01,  1.1702e+00,\n       -4.7248e-01,  4.2867e-01, -4.2025e-01,  2.4803e-01,  6.8194e-01,\n       -6.7488e-01,  9.2401e-02,  1.3089e+00, -3.6278e-02,  2.0098e-01,\n        7.6005e-01, -6.6718e-02, -7.7794e-02,  2.3844e-01, -2.4351e-01,\n       -5.4164e-01, -3.3540e-01,  2.9805e-01,  3.5269e-01, -8.0594e-01,\n       -4.3611e-01,  6.1535e-01,  3.4212e-01, -3.3603e-01,  3.3282e-01,\n        3.8065e-01,  5.7427e-02,  9.9918e-02,  1.2525e-01,  1.1039e+00,\n        3.6678e-02,  3.0490e-01, -1.4942e-01,  3.2912e-01,  2.3300e-01,\n        4.3395e-01,  1.5666e-01,  2.2778e-01, -2.5830e-02,  2.4334e-01,\n       -5.8136e-02, -1.3486e-01,  2.4521e-01, -3.3459e-01,  4.2839e-01,\n       -4.8181e-01,  1.3403e-01,  2.6049e-01,  8.9933e-02, -9.3770e-02,\n        3.7672e-01, -2.9558e-02,  4.3841e-01,  6.1212e-01, -2.5720e-01,\n       -7.8506e-01,  2.3880e-01,  1.3399e-01, -7.9315e-02,  7.0582e-01,\n        3.9968e-01,  6.7779e-01, -2.0474e-03,  1.9785e-02, -4.2059e-01,\n       -5.3858e-01, -5.2155e-02,  1.7252e-01,  2.7547e-01, -4.4482e-01,\n        2.3595e-01, -2.3445e-01,  3.0103e-01, -5.5096e-01, -3.1159e-02,\n       -3.4433e-01,  1.2386e+00,  1.0317e+00, -2.2728e-01, -9.5207e-03,\n       -2.5432e-01, -2.9792e-01,  2.5934e-01, -1.0421e-01, -3.3876e-01,\n        4.2470e-01,  5.8335e-04,  1.3093e-01,  2.8786e-01,  2.3474e-01,\n        2.5905e-02, -6.4359e-01,  6.1330e-02,  6.3842e-01,  1.4705e-01,\n       -6.1594e-01,  2.5097e-01, -4.4872e-01,  8.6825e-01,  9.9555e-02,\n       -4.4734e-02, -7.4239e-01, -5.9147e-01, -5.4929e-01,  3.8108e-01,\n        5.5177e-02, -1.0487e-01, -1.2838e-01,  6.0521e-03,  2.8743e-01,\n        2.1592e-01,  7.2871e-02, -3.1644e-01, -4.3321e-01,  1.8682e-01,\n        6.7274e-02,  2.8115e-01, -4.6222e-02, -9.6803e-02,  5.6091e-01,\n       -6.7762e-01, -1.6645e-01,  1.5553e-01,  5.2301e-01, -3.0058e-01,\n       -3.7291e-01,  8.7895e-02, -1.7963e-01, -4.4193e-01, -4.4607e-01,\n       -2.4122e+00,  3.3738e-01,  6.2416e-01,  4.2787e-01, -2.5386e-01,\n       -6.1683e-01, -7.0097e-01,  4.9303e-01,  3.6916e-01, -9.7499e-02,\n        6.1411e-01, -4.7572e-03,  4.3916e-01, -2.1551e-01, -5.6745e-01,\n       -4.0278e-01,  2.9459e-01, -3.0850e-01,  1.0103e-01,  7.9741e-02,\n       -6.3811e-01,  2.4781e-01, -4.4546e-01,  1.0828e-01, -2.3624e-01,\n       -5.0838e-01, -1.7001e-01, -7.8735e-01,  3.4073e-01, -3.1830e-01,\n        4.5286e-01, -9.5118e-02,  2.0772e-01, -8.0183e-02, -3.7982e-01,\n       -4.9949e-01,  4.0759e-02, -3.7724e-01, -8.9705e-02, -6.8187e-01,\n        2.2106e-01, -3.9931e-01,  3.2329e-01, -3.6180e-01, -7.2093e-01,\n       -6.3404e-01,  4.3125e-01, -4.9743e-01, -1.7395e-01, -3.8779e-01,\n       -3.2556e-01,  1.4423e-01, -8.3401e-02, -2.2994e-01,  2.7793e-01,\n        4.9112e-01,  6.4511e-01, -7.8945e-02,  1.1171e-01,  3.7264e-01,\n        1.3070e-01, -6.1607e-02, -4.3501e-01,  2.8999e-02,  5.6224e-01,\n        5.8012e-02,  4.7078e-02,  4.2770e-01,  7.3245e-01, -2.1150e-02,\n        1.1988e-01,  7.8823e-02, -1.9106e-01,  3.5278e-02, -3.1102e-01,\n        1.3209e-01, -2.8606e-01, -1.5649e-01, -6.4339e-01,  4.4599e-01,\n       -3.0912e-01,  4.4520e-01, -3.6774e-01,  2.7327e-01,  6.7833e-01,\n       -8.3830e-02, -4.5120e-01,  1.0754e-01, -4.5908e-01,  1.5095e-01,\n       -4.5856e-01,  3.4465e-01,  7.8013e-02, -2.8319e-01, -2.8149e-02,\n        2.4404e-01, -7.1345e-01,  5.2834e-02, -2.8085e-01,  2.5344e-02,\n        4.2979e-02,  1.5663e-01, -7.4647e-01, -1.1301e+00,  4.4135e-01,\n        3.1444e-01, -1.0018e-01, -5.3526e-01, -9.0601e-01, -6.4954e-01,\n        4.2664e-02, -7.9927e-02,  3.2905e-01, -3.0797e-01, -1.9190e-02,\n        4.2765e-01,  3.1460e-01,  2.9051e-01, -2.7386e-01,  6.8483e-01,\n        1.9395e-02, -3.2884e-01, -4.8239e-01, -1.5747e-01, -1.6036e-01,\n        4.9164e-01, -7.0352e-01, -3.5591e-01, -7.4887e-01, -5.2827e-01,\n        4.4983e-02,  5.9247e-02,  4.6224e-01,  8.9697e-02, -7.5618e-01,\n        6.3682e-01,  9.0680e-02,  6.8830e-02,  1.8296e-01,  1.0754e-01,\n        6.7811e-01, -1.4716e-01,  1.7029e-01, -5.2630e-01,  1.9268e-01,\n        9.3130e-01,  8.0363e-01,  6.1324e-01, -3.0494e-01,  2.0236e-01,\n        5.8520e-01,  2.6484e-01, -4.5863e-01,  2.1035e-03, -5.6990e-01,\n       -4.9092e-01,  4.2511e-01, -1.0954e+00,  1.7124e-01,  2.2495e-01],\n      dtype=float32)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Problem 1: Finding similar words"},{"metadata":{},"cell_type":"markdown","source":"Your first task is to use the word embeddings to find similar words. More specifically, we ask you to write a function `most_similar` that takes a vector $x$ and returns a list with the 10 most similar entries in spaCy&rsquo;s vocabulary, with similarity being defined by cosine.\n\n**Tip:** spaCy already has a [`most_similar`](https://spacy.io/api/vectors#most_similar) method that you can wrap."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Enter your implementation of `most_similar` here\nfrom scipy.spatial.distance import cosine\n\n# Not display warnings - Spacy's most_similar() prints a warning everytime\n# This is caused by the zero vector corresponding to the <OOV> token\n# Note: This issue is fixed in Spacy 2.2.2 (https://github.com/explosion/spaCy/issues/3412)\nimport warnings  \nwarnings.filterwarnings('ignore')\n\ndef most_similar(query_vector, n=10):\n    # n=n+1 because the <OOV> token with zero vector and similarity=nan is ignored\n    # So, most_similar() returns n-1 results instead of n\n    similar_tokens_hash = nlp.vocab.vectors.most_similar(query_vector.reshape([1,-1]), n=n+1)[0][0]\n    similar_words = [nlp.vocab[hash] for hash in similar_tokens_hash]\n    return similar_words","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test your implementation by running the following code cell, which will print the 10 most similar words for the word *cheese*:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.manifold import TSNE\nimport plotly.graph_objects as go\nimport pandas as pd\nimport seaborn as sns\nfrom bokeh.palettes import inferno, plasma, viridis","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" \".join(w.text for w in most_similar(nlp.vocab[\"cheese\"].vector)))","execution_count":6,"outputs":[{"output_type":"stream","text":"cheese CHEESE Cheese Cheddar CHEDDAR cheddar Bacon BACON bacon Cheeses\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"You should get the following output:"},{"metadata":{},"cell_type":"raw","source":"Cheese CHEESE cheese Cheddar cheddar CHEDDAR BACON Bacon bacon cheeses"},{"metadata":{},"cell_type":"markdown","source":"Once you have a working implementation of `most_similar`, use it to think about in what sense the returned words really are &lsquo;similar&rsquo; to the cue word. Try to find examples where the cue word and at least one of the words returned by `most_similar` are in the following semantic relations:\n\n1. synonymy (exchangeable meanings)\n2. antonymy (opposite meanings)\n3. hyperonymy/hyponymy (more specific/less specific meanings)\n\nDocument your examples in the code cell below."},{"metadata":{},"cell_type":"markdown","source":"After quite some playing around, we found the following examples for the above mentioned semantic relations."},{"metadata":{},"cell_type":"markdown","source":"#### Synonymy - Words with exchangeable meaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"def print_similar_words(query_words):\n    for word in query_words:\n        print(\"{} : {}\".format(word, \" | \".join(w.text for w in most_similar(nlp.vocab[word].vector))))\n        print(\"------------------------------------------------------------------\")","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Synonymy\nprint_similar_words([\"nice\", \"sadness\", \"beautiful\"])","execution_count":8,"outputs":[{"output_type":"stream","text":"nice : Nice | NIce | nice | nICE | NICE | Good | gOOD | GOOD | good | GOod\n------------------------------------------------------------------\nsadness : SADNESS | sadness | Sadness | Sorrow | SORROW | sorrow | DESPAIR | despair | Despair | GRIEF\n------------------------------------------------------------------\nbeautiful : beautiful | BEAUTIFUL | Beautiful | GORGEOUS | Gorgeous | gorgeous | lovely | Lovely | LOVELY | Stunning\n------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Antonymy - Words with opposite meaning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Antonymy\nprint_similar_words([\"positive\", \"advantages\", \"pros\"])","execution_count":9,"outputs":[{"output_type":"stream","text":"positive : positive | Positive | POSITIVE | negative | NEGATIVE | Negative | POSTIVE | postive | Postive | positivity\n------------------------------------------------------------------\nadvantages : ADVANTAGES | advantages | Advantages | Disadvantages | DISADVANTAGES | disadvantages | drawbacks | DRAWBACKS | Drawbacks | Advantage\n------------------------------------------------------------------\npros : PROs | Pros | pros | PROS | CONs | CONS | Cons | cons | PRO | prO\n------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperonymy - Words which are a specific type of query word"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperonymy\nprint_similar_words([\"wool\", \"metal\", \"literature\", \"footwear\"])","execution_count":10,"outputs":[{"output_type":"stream","text":"wool : wool | WOOL | Wool | cashmere | Cashmere | CASHMERE | merino | MERINO | Merino | woolen\n------------------------------------------------------------------\nmetal : Metal | METAL | metal | Steel | steel | STEEL | Aluminum | ALUMINUM | aluminum | IRON\n------------------------------------------------------------------\nliterature : LITERATURE | Literature | literature | LITERARY | Literary | literary | poetry | Poetry | POETRY | scholarly\n------------------------------------------------------------------\nfootwear : Footwear | footwear | FOOTWEAR | Shoes | shoes | SHOES | shoe | Shoe | SHOE | Sneakers\n------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Hyponymy - Words which are a general class to which query word belongs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyponymy\nprint_similar_words([\"shoe\", \"arachnophobia\", \"elk\"])","execution_count":11,"outputs":[{"output_type":"stream","text":"shoe : shoe | SHOE | Shoe | shoes | SHOES | Shoes | Footwear | footwear | FOOTWEAR | SNEAKER\n------------------------------------------------------------------\narachnophobia : Arachnophobia | arachnophobia | acrophobia | ACROPHOBIA | Acrophobia | agoraphobia | Agoraphobia | Phobia | phobia | PHOBIA\n------------------------------------------------------------------\nelk : Elk | elk | ELK | deer | Deer | DEER | Moose | MOOSE | moose | Bison\n------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Problem 2: Plotting similar words"},{"metadata":{},"cell_type":"markdown","source":"Your next task is to visualize the word embedding space by a plot. To do so, you will have to reduce the dimensionality of the space from 300 to 2&nbsp;dimensions. One suitable algorithm for this is [T-distributed Stochastic Neighbor Embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (TSNE), which is implemented in scikit-learn&rsquo;s [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) class.\n\nWrite a function `plot_most_similar` that takes a list of words (lexemes) and does the following:\n\n1. For each word in the list, find the most similar words (lexemes) in the spaCy vocabulary.\n2. Compute the TSNE transformation of the corresponding vectors to 2&nbsp;dimensions.\n3. Produce a scatter plot of the transformed vectors, with the vectors as points and the corresponding word forms as labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Write code here to plot the most similar words\ndef plot_most_similar(target):\n    \n    unique_ref = [ref.text for ref in target]\n    similar_lexemes = [most_similar(nlp.vocab[x].vector) for x in unique_ref]\n    array_lex = np.array([w.vector for list_lex in similar_lexemes for w in list_lex])\n    ref_words = np.concatenate([np.repeat(ref, len(similar_lexemes[0])) for ref in unique_ref])\n    df_plot = pd.DataFrame(TSNE(n_components=2).fit_transform(array_lex))\n    df_plot[2] = [x.text for list_lex in similar_lexemes for x in list_lex]\n    df_plot.columns = [\"Dim1\", \"Dim2\", \"Word\"]\n    df_plot[3] = np.concatenate([np.repeat(x, len(similar_lexemes[0])) for x in viridis(len(similar_lexemes))])\n    df_plot[4] = [\"Coordinates: (\" + str(round(df_plot[\"Dim1\"][i], 4)) + \"; \" + str(round(df_plot[\"Dim2\"][i], 4))\n                  + \")<br>Word: \" + df_plot[\"Word\"][i] + \"<br>Reference: \" + ref_words[i] \n                  for i in range(df_plot.shape[0])]\n    df_plot.columns = [\"Dim1\", \"Dim2\", \"Word\", \"Color\", \"Hover\"]\n    \n    \n    # Plot with plotly\n    fig = go.Figure()\n\n    for i in range(len(unique_ref)):\n        df_temp = df_plot[(i*len(similar_lexemes[0])):(i*len(similar_lexemes[0])+len(similar_lexemes[0]))]\n        \n        fig.add_trace(go.Scatter(\n            x=df_temp[\"Dim1\"],\n            y=df_temp[\"Dim2\"],\n            mode=\"markers+text\",\n            text=df_temp[\"Word\"],\n            textposition=\"bottom center\",\n            textfont=dict(\n                family=\"sans serif\",\n                size=14,\n                color=\"steelblue\"\n            ),\n            hovertext = df_temp[\"Hover\"],\n            hoverinfo=\"text\",\n            marker=dict(\n                size=16,\n                cmax=39,\n                cmin=0,\n                color=df_temp[\"Color\"],\n                colorscale=\"Viridis\"\n            ),\n            name=unique_ref[i],\n            showlegend=True\n        ))\n\n    fig.update_xaxes(zeroline=True, zerolinewidth=1, zerolinecolor='black')\n    fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor='black')\n    fig.update_layout(\n        title = \"2D TSNE of the sample words and their top 10 most similar\",\n        xaxis_title=\"Dimension 1\",\n        yaxis_title=\"Dimension 2\",\n        font=dict(\n            family=\"Courier New, monospace\",\n            size=18,\n            color=\"#7f7f7f\"\n        )\n    )\n\n    fig.show()","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test your code by running the following cell:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_most_similar(nlp.vocab[w] for w in [\"cheese\", \"goat\", \"sweden\", \"university\", \"computer\"])","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>\n        \n        \n            <div id=\"538cdb71-3c33-4c31-b7fa-b2ba7b076291\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                require([\"plotly\"], function(Plotly) {\n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    \n                if (document.getElementById(\"538cdb71-3c33-4c31-b7fa-b2ba7b076291\")) {\n                    Plotly.newPlot(\n                        '538cdb71-3c33-4c31-b7fa-b2ba7b076291',\n                        [{\"hoverinfo\": \"text\", \"hovertext\": [\"Coordinates: (-8.3101; 22.748)<br>Word: cheese<br>Reference: cheese\", \"Coordinates: (-14.8338; 92.6342)<br>Word: CHEESE<br>Reference: cheese\", \"Coordinates: (-31.624; 110.2109)<br>Word: Cheese<br>Reference: cheese\", \"Coordinates: (-51.7625; 132.8126)<br>Word: Cheddar<br>Reference: cheese\", \"Coordinates: (-59.3321; 98.3012)<br>Word: CHEDDAR<br>Reference: cheese\", \"Coordinates: (-74.8328; 119.3844)<br>Word: cheddar<br>Reference: cheese\", \"Coordinates: (-3.1831; 121.2201)<br>Word: Bacon<br>Reference: cheese\", \"Coordinates: (-18.9313; 143.3186)<br>Word: BACON<br>Reference: cheese\", \"Coordinates: (9.1478; 143.2978)<br>Word: bacon<br>Reference: cheese\", \"Coordinates: (-36.8953; 73.4614)<br>Word: Cheeses<br>Reference: cheese\"], \"marker\": {\"cmax\": 39, \"cmin\": 0, \"color\": [\"#440154\", \"#440154\", \"#440154\", \"#440154\", \"#440154\", \"#440154\", \"#440154\", \"#440154\", \"#440154\", \"#440154\"], \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"size\": 16}, \"mode\": \"markers+text\", \"name\": \"cheese\", \"showlegend\": true, \"text\": [\"cheese\", \"CHEESE\", \"Cheese\", \"Cheddar\", \"CHEDDAR\", \"cheddar\", \"Bacon\", \"BACON\", \"bacon\", \"Cheeses\"], \"textfont\": {\"color\": \"steelblue\", \"family\": \"sans serif\", \"size\": 14}, \"textposition\": \"bottom center\", \"type\": \"scatter\", \"x\": [-8.310117721557617, -14.833845138549805, -31.623958587646484, -51.76245880126953, -59.33206558227539, -74.83284759521484, -3.183122158050537, -18.93132209777832, 9.14781379699707, -36.89528274536133], \"y\": [22.748046875, 92.63423919677734, 110.21092987060547, 132.8126220703125, 98.30121612548828, 119.38442993164062, 121.2200927734375, 143.31858825683594, 143.29774475097656, 73.46135711669922]}, {\"hoverinfo\": \"text\", \"hovertext\": [\"Coordinates: (14.8103; 12.2108)<br>Word: goat<br>Reference: goat\", \"Coordinates: (22.5519; 93.0005)<br>Word: Goat<br>Reference: goat\", \"Coordinates: (45.1911; 105.5095)<br>Word: GOAT<br>Reference: goat\", \"Coordinates: (39.7206; 70.9505)<br>Word: GOATs<br>Reference: goat\", \"Coordinates: (65.0359; 81.6354)<br>Word: GOATS<br>Reference: goat\", \"Coordinates: (30.1187; 47.129)<br>Word: goats<br>Reference: goat\", \"Coordinates: (11.306; 63.5348)<br>Word: Goats<br>Reference: goat\", \"Coordinates: (62.4568; 51.3024)<br>Word: cow<br>Reference: goat\", \"Coordinates: (83.3762; 39.4145)<br>Word: COW<br>Reference: goat\", \"Coordinates: (55.8498; 26.8851)<br>Word: Cow<br>Reference: goat\"], \"marker\": {\"cmax\": 39, \"cmin\": 0, \"color\": [\"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\", \"#3B518A\"], \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"size\": 16}, \"mode\": \"markers+text\", \"name\": \"goat\", \"showlegend\": true, \"text\": [\"goat\", \"Goat\", \"GOAT\", \"GOATs\", \"GOATS\", \"goats\", \"Goats\", \"cow\", \"COW\", \"Cow\"], \"textfont\": {\"color\": \"steelblue\", \"family\": \"sans serif\", \"size\": 14}, \"textposition\": \"bottom center\", \"type\": \"scatter\", \"x\": [14.810311317443848, 22.551908493041992, 45.191097259521484, 39.720558166503906, 65.03592681884766, 30.118656158447266, 11.305994987487793, 62.45684814453125, 83.37617492675781, 55.84978103637695], \"y\": [12.21076774597168, 93.0004653930664, 105.50951385498047, 70.95048522949219, 81.63544464111328, 47.128990173339844, 63.53485107421875, 51.30244064331055, 39.414466857910156, 26.885082244873047]}, {\"hoverinfo\": \"text\", \"hovertext\": [\"Coordinates: (-58.0277; -62.7787)<br>Word: SWEDEN<br>Reference: sweden\", \"Coordinates: (-6.8773; -67.0906)<br>Word: Sweden<br>Reference: sweden\", \"Coordinates: (19.6172; -11.8023)<br>Word: sweden<br>Reference: sweden\", \"Coordinates: (-32.9549; -79.7598)<br>Word: FINLAND<br>Reference: sweden\", \"Coordinates: (-31.3804; -53.5751)<br>Word: Finland<br>Reference: sweden\", \"Coordinates: (-13.5484; -96.2237)<br>Word: finland<br>Reference: sweden\", \"Coordinates: (-55.2624; -92.6406)<br>Word: NORWAY<br>Reference: sweden\", \"Coordinates: (-70.194; -113.7812)<br>Word: Norway<br>Reference: sweden\", \"Coordinates: (-81.2919; -83.344)<br>Word: norway<br>Reference: sweden\", \"Coordinates: (-35.9759; -116.3183)<br>Word: Denmark<br>Reference: sweden\"], \"marker\": {\"cmax\": 39, \"cmin\": 0, \"color\": [\"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\", \"#208F8C\"], \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"size\": 16}, \"mode\": \"markers+text\", \"name\": \"sweden\", \"showlegend\": true, \"text\": [\"SWEDEN\", \"Sweden\", \"sweden\", \"FINLAND\", \"Finland\", \"finland\", \"NORWAY\", \"Norway\", \"norway\", \"Denmark\"], \"textfont\": {\"color\": \"steelblue\", \"family\": \"sans serif\", \"size\": 14}, \"textposition\": \"bottom center\", \"type\": \"scatter\", \"x\": [-58.027713775634766, -6.877347946166992, 19.617197036743164, -32.95490646362305, -31.380441665649414, -13.548417091369629, -55.26243209838867, -70.19400787353516, -81.29186248779297, -35.975852966308594], \"y\": [-62.778656005859375, -67.0905990600586, -11.802268981933594, -79.75977325439453, -53.57510757446289, -96.22367095947266, -92.6406478881836, -113.78119659423828, -83.34394836425781, -116.31827545166016]}, {\"hoverinfo\": \"text\", \"hovertext\": [\"Coordinates: (72.157; -51.1557)<br>Word: University<br>Reference: university\", \"Coordinates: (-0.535; -25.2611)<br>Word: university<br>Reference: university\", \"Coordinates: (89.6664; -70.255)<br>Word: UNIVERSITY<br>Reference: university\", \"Coordinates: (62.5509; -23.8716)<br>Word: COLLEGE<br>Reference: university\", \"Coordinates: (98.5686; -36.3517)<br>Word: College<br>Reference: university\", \"Coordinates: (86.199; -14.2314)<br>Word: college<br>Reference: university\", \"Coordinates: (61.374; -99.1736)<br>Word: Universities<br>Reference: university\", \"Coordinates: (34.7917; -82.7311)<br>Word: UNIVERSITIES<br>Reference: university\", \"Coordinates: (57.793; -74.5541)<br>Word: universities<br>Reference: university\", \"Coordinates: (39.9112; -51.0386)<br>Word: colleges<br>Reference: university\"], \"marker\": {\"cmax\": 39, \"cmin\": 0, \"color\": [\"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\", \"#5BC862\"], \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"size\": 16}, \"mode\": \"markers+text\", \"name\": \"university\", \"showlegend\": true, \"text\": [\"University\", \"university\", \"UNIVERSITY\", \"COLLEGE\", \"College\", \"college\", \"Universities\", \"UNIVERSITIES\", \"universities\", \"colleges\"], \"textfont\": {\"color\": \"steelblue\", \"family\": \"sans serif\", \"size\": 14}, \"textposition\": \"bottom center\", \"type\": \"scatter\", \"x\": [72.15701293945312, -0.5349887609481812, 89.66642761230469, 62.55092239379883, 98.56864166259766, 86.19898223876953, 61.37403106689453, 34.79167556762695, 57.79301452636719, 39.91123962402344], \"y\": [-51.15566635131836, -25.261144638061523, -70.2549819946289, -23.871597290039062, -36.3516731262207, -14.23137378692627, -99.173583984375, -82.73110961914062, -74.55406188964844, -51.0385627746582]}, {\"hoverinfo\": \"text\", \"hovertext\": [\"Coordinates: (-66.948; 29.7031)<br>Word: COMPUTER<br>Reference: computer\", \"Coordinates: (-10.7015; -2.8066)<br>Word: computer<br>Reference: computer\", \"Coordinates: (-48.9776; -15.8933)<br>Word: Computer<br>Reference: computer\", \"Coordinates: (-73.4183; 2.3989)<br>Word: computers<br>Reference: computer\", \"Coordinates: (-77.5863; -21.9817)<br>Word: Computers<br>Reference: computer\", \"Coordinates: (-48.1999; 10.3366)<br>Word: COMPUTERS<br>Reference: computer\", \"Coordinates: (-126.2898; 13.9917)<br>Word: Laptop<br>Reference: computer\", \"Coordinates: (-102.2733; 23.1683)<br>Word: LAPTOP<br>Reference: computer\", \"Coordinates: (-109.5187; -6.0485)<br>Word: laptop<br>Reference: computer\", \"Coordinates: (16.2815; -127.1996)<br>Word: software<br>Reference: computer\"], \"marker\": {\"cmax\": 39, \"cmin\": 0, \"color\": [\"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\", \"#FDE724\"], \"colorscale\": [[0.0, \"#440154\"], [0.1111111111111111, \"#482878\"], [0.2222222222222222, \"#3e4989\"], [0.3333333333333333, \"#31688e\"], [0.4444444444444444, \"#26828e\"], [0.5555555555555556, \"#1f9e89\"], [0.6666666666666666, \"#35b779\"], [0.7777777777777778, \"#6ece58\"], [0.8888888888888888, \"#b5de2b\"], [1.0, \"#fde725\"]], \"size\": 16}, \"mode\": \"markers+text\", \"name\": \"computer\", \"showlegend\": true, \"text\": [\"COMPUTER\", \"computer\", \"Computer\", \"computers\", \"Computers\", \"COMPUTERS\", \"Laptop\", \"LAPTOP\", \"laptop\", \"software\"], \"textfont\": {\"color\": \"steelblue\", \"family\": \"sans serif\", \"size\": 14}, \"textposition\": \"bottom center\", \"type\": \"scatter\", \"x\": [-66.94800567626953, -10.701531410217285, -48.977630615234375, -73.4183120727539, -77.5863037109375, -48.199851989746094, -126.28982543945312, -102.27330017089844, -109.51868438720703, 16.281503677368164], \"y\": [29.70313262939453, -2.8066132068634033, -15.89330768585205, 2.3988516330718994, -21.981719970703125, 10.336625099182129, 13.991695404052734, 23.168264389038086, -6.048520565032959, -127.1995620727539]}],\n                        {\"font\": {\"color\": \"#7f7f7f\", \"family\": \"Courier New, monospace\", \"size\": 18}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"2D TSNE of the sample words and their top 10 most similar\"}, \"xaxis\": {\"title\": {\"text\": \"Dimension 1\"}, \"zeroline\": true, \"zerolinecolor\": \"black\", \"zerolinewidth\": 1}, \"yaxis\": {\"title\": {\"text\": \"Dimension 2\"}, \"zeroline\": true, \"zerolinecolor\": \"black\", \"zerolinewidth\": 1}},\n                        {\"responsive\": true}\n                    ).then(function(){\n                            \nvar gd = document.getElementById('538cdb71-3c33-4c31-b7fa-b2ba7b076291');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })\n                };\n                });\n            </script>\n        </div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Problem 3: Analogies"},{"metadata":{},"cell_type":"markdown","source":"In a **word analogy task** you are given three words $x$, $y$, $z$ and have to predict a word $w$ that has the same semantic relation to $z$ as $y$ has to $x$. One example is *man*, *woman*, *brother*, the expected answer being *sister* (the semantic relation is *male*/*female*).\n\n[Mikolov et al. (2013)](http://www.aclweb.org/anthology/N13-1090) have shown that word analogy tasks can be solved by adding and substracting word vectors in a word embedding: the vector for *sister* is the closest vector (in terms of cosine distance) to the vector *brother* $-$ *man* $+$ *woman*. Your next task is to write a function `fourth` that takes in three words (say *brother*, *man*, *woman*) and predicts the word that completes the analogy (in this case, *sister*)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Enter code here to solve the analogy problem\ndef analogy(x, y, z):\n    \n    # First we find the ideal value of the vector for the unknown word w\n    # THIS IS NOT WORKING?????? From the paper this is supposed to be the right computation...\n    # But we got words similar to z!\n    # w_ideal = y.vector - x.vector + z.vector\n    w_ideal = x.vector - y.vector + z.vector\n    \n    # Since this exact vector does not exist, we need to find the one with highest cosine similarity\n    # among all the terms present in our vocabulary.\n    # Having a dictionary available, we find the most similar to the ideal one \n    w = most_similar(w_ideal, n = 1)[0]\n    \n    return(w)\n\ndef print_analogy(x, y, z, expected = False):\n    if bool(expected):\n        print(\"Analogy for: \", x, \" - \", y, \" + \", z, \" (expected \", expected, \")\", sep = \"\")\n    else:\n        print(\"Analogy for:\", x, \"-\", y, \"+\", z)\n    print(analogy(nlp.vocab[x], nlp.vocab[y], nlp.vocab[z]).text)\n    print(\"--------------------------------------------------------------------------------\")","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test your code by running the following code. You should get *sister*."},{"metadata":{"trusted":true},"cell_type":"code","source":"print_analogy(\"brother\", \"man\", \"woman\", expected = \"sister\")","execution_count":15,"outputs":[{"output_type":"stream","text":"Analogy for: brother - man + woman (expected sister)\nsister\n--------------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"You should also be able to get the following:\n\n* *Stockholm* $-$ *Sweden* $+$ *Germany* $=$ *Berlin*\n* *Swedish* $-$ *Sweden* $+$ *France* $=$ *French*\n* *better* $-$ *good* $+$ *bad* $=$ *worse*\n* *walked* $-$ *walk* $+$ *take* $=$ *took*\n\nExperiment with other examples to see whether you get the expected output. Provide three examples of analogies for which the model produces the &lsquo;correct&rsquo; answer, and three examples on which the model &lsquo;failed&rsquo;. Based on your theoretical understanding of word embeddings, do you have a hypothesis as to why the model succeeds/fails in completing the analogy? Discuss this question in a short text."},{"metadata":{},"cell_type":"markdown","source":"#### Given analogies"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_analogy(\"Stockholm\", \"Sweden\", \"Germany\")\nprint_analogy(\"Swedish\", \"Sweden\", \"France\")\nprint_analogy(\"better\", \"good\", \"bad\")\nprint_analogy(\"walked\", \"walk\", \"take\")","execution_count":16,"outputs":[{"output_type":"stream","text":"Analogy for: Stockholm - Sweden + Germany\nBerlin\n--------------------------------------------------------------------------------\nAnalogy for: Swedish - Sweden + France\nfrench\n--------------------------------------------------------------------------------\nAnalogy for: better - good + bad\nWorse\n--------------------------------------------------------------------------------\nAnalogy for: walked - walk + take\ntook\n--------------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Experimental analogies - Successful examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_analogy(\"Sweden\", \"Swedish\", \"Spanish\", expected = \"Spain\")\nprint_analogy(\"Swedish\", \"Sweden\", \"Italy\", expected = \"Italian\")\nprint_analogy(\"feel\", \"felt\", \"made\", expected = \"make\")","execution_count":17,"outputs":[{"output_type":"stream","text":"Analogy for: Sweden - Swedish + Spanish (expected Spain)\nSpain\n--------------------------------------------------------------------------------\nAnalogy for: Swedish - Sweden + Italy (expected Italian)\nitalian\n--------------------------------------------------------------------------------\nAnalogy for: feel - felt + made (expected make)\nMAKE\n--------------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Experimental analogies - Failed examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"print_analogy(\"Sweden\", \"Swedish\", \"Norway\", expected = \"Norvegian\")\nprint_analogy(\"mocassin\", \"shoe\", \"trousers\", expected = \"jeans or similar\")\nprint_analogy(\"hand\", \"arm\", \"leg\", expected = \"feet/foot\")","execution_count":18,"outputs":[{"output_type":"stream","text":"Analogy for: Sweden - Swedish + Norway (expected Norvegian)\nNorway\n--------------------------------------------------------------------------------\nAnalogy for: mocassin - shoe + trousers (expected jeans or similar)\ntrousers\n--------------------------------------------------------------------------------\nAnalogy for: hand - arm + leg (expected feet/foot)\nleg\n--------------------------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**ANSWER:**  \nAs we can see, the effectivness of the word embedding is limited: only a few examples are actually working, and that does not even depend on the semantic field (we have both successful and failing examples \nin countries-nationalities for instance).  \nThe failing examples we reported show another important limit of the algorithm: it failes to recognize non-unique analogies (like with the trousers example) and analogies that present commonly used variations of the same words (like feet/foot). The succesful examples that we were able to find were only among the semantic fields already present in this lab.  \nWe hypothesize that one of the reasons behind this lack of precision is the possibility of the vectors *x* and *y* to cancel each other out (due to the often high similarity of the two). In fact we often get back exactly the same word of *z*. Another thing to take into account is that this algorithm does not exclude from the pool of analogies the same words of *x*, *y* and *z*."},{"metadata":{},"cell_type":"markdown","source":"## Natural language inference dataset"},{"metadata":{},"cell_type":"markdown","source":"In the second part of this lab, you will be evaluating the usefulness of word embeddings in the context of a natural language inference task. The data for this part is the [SNLI corpus](https://nlp.stanford.edu/projects/snli/), a collection of 570k human-written English image caption pairs manually labeled with the labels *Entailment*, *Contradiction*, and *Neutral*. Consider the following sentence pair as an example:\n\n* Sentence 1: A soccer game with multiple males playing.\n* Sentence 2: Some men are playing a sport.\n\nThis pair is labeled with *Entailment*, because sentence&nbsp;2 is logically entailed (implied) by sentence&nbsp;1 – if sentence&nbsp;1 is true, then sentence&nbsp;2 is true, too. The following sentence pair, on the other hand, is labeled with *Contradiction*, because both sentences cannot be true at the same time.\n\n* Sentence 1: A black race car starts up in front of a crowd of people.\n* Sentence 2: A man is driving down a lonely road.\n\nFor detailed information about the corpus, refer to [Bowman et al. (2015)](https://www.aclweb.org/anthology/D15-1075/). For this lab, we load the training portion and the development portion of the dataset.\n\n**Note:** Because the SNLI corpus is rather big, we initially only load a small portion (25,000 samples) of the training data. Once you have working code for Problems&nbsp;4–6, you should set the flag `final` to `True` and re-run all cells with the full dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"import bz2\nimport pandas as pd\n\nfinal_evaluation = True    # TODO: Set to True for the final evaluation!\n\nwith bz2.open(\"../input/db-text-mining-lab4/train.jsonl.bz2\", 'rt') as source:\n    if final_evaluation:\n        df_train = pd.read_json(source, lines=True)\n    else:\n        df_train = pd.read_json(source, lines=True)[:25000]\n    print(\"Number of sentence pairs in the training data:\", len(df_train))\n\nwith bz2.open(\"../input/db-text-mining-lab4/dev.jsonl.bz2\", 'rt') as source:\n    df_dev = pd.read_json(source, lines=True)\n    print(\"Number of sentence pairs in the development data:\", len(df_dev))","execution_count":25,"outputs":[{"output_type":"stream","text":"Number of sentence pairs in the training data: 549367\nNumber of sentence pairs in the development data: 9842\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"When you inspect the data frames, you will see that we have preprocessed the sentences and separated tokens by spaces. In the columns `tagged1` and `tagged2`, we have added the part-of-speech tags for every token (as predicted by spaCy), also separated by spaces."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"      gold_label                                          sentence1  \\\n0        neutral  A person on a horse jumps over a broken down a...   \n1  contradiction  A person on a horse jumps over a broken down a...   \n2     entailment  A person on a horse jumps over a broken down a...   \n3        neutral              Children smiling and waving at camera   \n4     entailment              Children smiling and waving at camera   \n\n                                               tags1  \\\n0  DET NOUN ADP DET NOUN VERB ADP DET ADJ ADP NOU...   \n1  DET NOUN ADP DET NOUN VERB ADP DET ADJ ADP NOU...   \n2  DET NOUN ADP DET NOUN VERB ADP DET ADJ ADP NOU...   \n3                      NOUN VERB CCONJ VERB ADP NOUN   \n4                      NOUN VERB CCONJ VERB ADP NOUN   \n\n                                           sentence2  \\\n0  A person is training his horse for a competiti...   \n1    A person is at a diner , ordering an omelette .   \n2                A person is outdoors , on a horse .   \n3                  They are smiling at their parents   \n4                         There are children present   \n\n                                               tags2  \n0     DET NOUN AUX VERB PRON NOUN ADP DET NOUN PUNCT  \n1  DET NOUN AUX ADP DET NOUN PUNCT VERB DET NOUN ...  \n2          DET NOUN AUX ADV PUNCT ADP DET NOUN PUNCT  \n3                        PRON AUX VERB ADP PRON NOUN  \n4                                  PRON AUX NOUN ADJ  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gold_label</th>\n      <th>sentence1</th>\n      <th>tags1</th>\n      <th>sentence2</th>\n      <th>tags2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>A person on a horse jumps over a broken down a...</td>\n      <td>DET NOUN ADP DET NOUN VERB ADP DET ADJ ADP NOU...</td>\n      <td>A person is training his horse for a competiti...</td>\n      <td>DET NOUN AUX VERB PRON NOUN ADP DET NOUN PUNCT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>contradiction</td>\n      <td>A person on a horse jumps over a broken down a...</td>\n      <td>DET NOUN ADP DET NOUN VERB ADP DET ADJ ADP NOU...</td>\n      <td>A person is at a diner , ordering an omelette .</td>\n      <td>DET NOUN AUX ADP DET NOUN PUNCT VERB DET NOUN ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>entailment</td>\n      <td>A person on a horse jumps over a broken down a...</td>\n      <td>DET NOUN ADP DET NOUN VERB ADP DET ADJ ADP NOU...</td>\n      <td>A person is outdoors , on a horse .</td>\n      <td>DET NOUN AUX ADV PUNCT ADP DET NOUN PUNCT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neutral</td>\n      <td>Children smiling and waving at camera</td>\n      <td>NOUN VERB CCONJ VERB ADP NOUN</td>\n      <td>They are smiling at their parents</td>\n      <td>PRON AUX VERB ADP PRON NOUN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>entailment</td>\n      <td>Children smiling and waving at camera</td>\n      <td>NOUN VERB CCONJ VERB ADP NOUN</td>\n      <td>There are children present</td>\n      <td>PRON AUX NOUN ADJ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Problem 4: Two simple baselines"},{"metadata":{},"cell_type":"markdown","source":"Your first task is to establish two simple baselines for the natural language inference task."},{"metadata":{},"cell_type":"markdown","source":"### Random baseline\n\nOne drawback with the Most Frequent Class (MFC) baseline is that it does not yield well-defined precision and recall values for all classes. Here we therefore ask you to implement a classifier that generates *random* predictions, where the probability of a class is determined by its relative frequency in the training data. This functionality is provided by scikit-learn&rsquo;s [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html). Write code to evaluate the performance of this classifier on the development data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.base import clone\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport itertools\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.externals import joblib","execution_count":27,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning:\n\nsklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to get predictions from given model for train and test data\ndef get_predictions(model, datasets):\n    return (model.predict(dataset) for dataset in datasets)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Enter code here to implement the random baseline. Print the classification report.\nnp.random.seed(10)\nmfc = DummyClassifier().fit(df_train.loc[:, df_train.columns!=\"gold_label\"], df_train[\"gold_label\"])\ntrain_preds, dev_preds = get_predictions(mfc, [df_train.loc[:, df_train.columns!=\"gold_label\"], \n                                               df_dev.loc[:, df_train.columns!=\"gold_label\"]])","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(train_preds, df_train[\"gold_label\"], output_dict=True))","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"           contradiction     entailment        neutral  accuracy  \\\nprecision       0.334418       0.333101       0.332494  0.333338   \nrecall          0.333029       0.333951       0.333036  0.333338   \nf1-score        0.333722       0.333525       0.332765  0.333338   \nsupport    183951.000000  182949.000000  182467.000000  0.333338   \n\n               macro avg   weighted avg  \nprecision       0.333338       0.333340  \nrecall          0.333338       0.333338  \nf1-score        0.333337       0.333339  \nsupport    549367.000000  549367.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.334418</td>\n      <td>0.333101</td>\n      <td>0.332494</td>\n      <td>0.333338</td>\n      <td>0.333338</td>\n      <td>0.333340</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.333029</td>\n      <td>0.333951</td>\n      <td>0.333036</td>\n      <td>0.333338</td>\n      <td>0.333338</td>\n      <td>0.333338</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.333722</td>\n      <td>0.333525</td>\n      <td>0.332765</td>\n      <td>0.333338</td>\n      <td>0.333337</td>\n      <td>0.333339</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>183951.000000</td>\n      <td>182949.000000</td>\n      <td>182467.000000</td>\n      <td>0.333338</td>\n      <td>549367.000000</td>\n      <td>549367.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(dev_preds, df_dev[\"gold_label\"], output_dict=True))","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"           contradiction   entailment      neutral  accuracy    macro avg  \\\nprecision       0.350824     0.343346     0.340649   0.34495     0.344940   \nrecall          0.347642     0.347839     0.339286   0.34495     0.344922   \nf1-score        0.349226     0.345578     0.339966   0.34495     0.344923   \nsupport      3308.000000  3286.000000  3248.000000   0.34495  9842.000000   \n\n           weighted avg  \nprecision      0.344969  \nrecall         0.344950  \nf1-score       0.344952  \nsupport     9842.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.350824</td>\n      <td>0.343346</td>\n      <td>0.340649</td>\n      <td>0.34495</td>\n      <td>0.344940</td>\n      <td>0.344969</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.347642</td>\n      <td>0.347839</td>\n      <td>0.339286</td>\n      <td>0.34495</td>\n      <td>0.344922</td>\n      <td>0.344950</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.349226</td>\n      <td>0.345578</td>\n      <td>0.339966</td>\n      <td>0.34495</td>\n      <td>0.344923</td>\n      <td>0.344952</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>3308.000000</td>\n      <td>3286.000000</td>\n      <td>3248.000000</td>\n      <td>0.34495</td>\n      <td>9842.000000</td>\n      <td>9842.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"For the DummyClassifier, we got a training accuracy and test accuracy of 33.4%\nThis makes sense as we have 3 classes and the data is almost equally distributed across all the classes."},{"metadata":{},"cell_type":"markdown","source":"### One-sided baseline\n\nA second obvious baseline for the inference task is to predict the class label of a sentence pair based on the text of only one of the two sentences, just as in a standard document classification task. Put together a simple [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) + [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) pipeline that implements this idea, train it, and evaluate it on the development data. Is it better to base predictions on sentence&nbsp;1 or sentence&nbsp;2?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Enter code here to implement the one-sentence baselines. Print the classification reports.\ncnt_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=4, max_df=0.9)\nlogreg = LogisticRegression(solver=\"saga\", multi_class=\"ovr\", max_iter=50)\n\nlogcnt_pipeline = Pipeline([(\"cnt_vectorizer\", cnt_vectorizer), (\"logreg\", logreg)])\n\nnp.random.seed(10)\nlogcnt_sent1_model = clone(logcnt_pipeline).fit(df_train[\"sentence1\"], df_train[\"gold_label\"])\n\nnp.random.seed(10)\nlogcnt_sent2_model = clone(logcnt_pipeline).fit(df_train[\"sentence2\"], df_train[\"gold_label\"])","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get predictions for train and test data from model with sentence1\ntrain_preds_1, test_preds_1 = get_predictions(logcnt_sent1_model, [df_train[\"sentence1\"], df_dev[\"sentence1\"]])\n\n# Get predictions for train and test data from model with sentence2\ntrain_preds_2, test_preds_2 = get_predictions(logcnt_sent2_model, [df_train[\"sentence2\"], df_dev[\"sentence2\"]])","execution_count":33,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### CountVectorizer + LogisticRegression on *sentence1*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_train[\"gold_label\"], train_preds_1, output_dict=True))","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"           contradiction     entailment        neutral  accuracy  \\\nprecision       0.337047       0.338963       0.338638  0.338346   \nrecall          0.259287       0.362209       0.393639  0.338346   \nf1-score        0.293097       0.350201       0.364073  0.338346   \nsupport    183187.000000  183416.000000  182764.000000  0.338346   \n\n               macro avg   weighted avg  \nprecision       0.338216       0.338216  \nrecall          0.338378       0.338346  \nf1-score        0.335790       0.335774  \nsupport    549367.000000  549367.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.337047</td>\n      <td>0.338963</td>\n      <td>0.338638</td>\n      <td>0.338346</td>\n      <td>0.338216</td>\n      <td>0.338216</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.259287</td>\n      <td>0.362209</td>\n      <td>0.393639</td>\n      <td>0.338346</td>\n      <td>0.338378</td>\n      <td>0.338346</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.293097</td>\n      <td>0.350201</td>\n      <td>0.364073</td>\n      <td>0.338346</td>\n      <td>0.335790</td>\n      <td>0.335774</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>183187.000000</td>\n      <td>183416.000000</td>\n      <td>182764.000000</td>\n      <td>0.338346</td>\n      <td>549367.000000</td>\n      <td>549367.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_dev[\"gold_label\"], test_preds_1, output_dict=True))","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"           contradiction   entailment      neutral  accuracy    macro avg  \\\nprecision       0.334370     0.341063     0.328870  0.334688     0.334768   \nrecall          0.262355     0.362271     0.379598  0.334688     0.334741   \nf1-score        0.294017     0.351347     0.352418  0.334688     0.332594   \nsupport      3278.000000  3329.000000  3235.000000  0.334688  9842.000000   \n\n           weighted avg  \nprecision      0.334826  \nrecall         0.334688  \nf1-score       0.332605  \nsupport     9842.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.334370</td>\n      <td>0.341063</td>\n      <td>0.328870</td>\n      <td>0.334688</td>\n      <td>0.334768</td>\n      <td>0.334826</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.262355</td>\n      <td>0.362271</td>\n      <td>0.379598</td>\n      <td>0.334688</td>\n      <td>0.334741</td>\n      <td>0.334688</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.294017</td>\n      <td>0.351347</td>\n      <td>0.352418</td>\n      <td>0.334688</td>\n      <td>0.332594</td>\n      <td>0.332605</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>3278.000000</td>\n      <td>3329.000000</td>\n      <td>3235.000000</td>\n      <td>0.334688</td>\n      <td>9842.000000</td>\n      <td>9842.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### CountVectorizer + LogisticRegression on *sentence2*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_train[\"gold_label\"], train_preds_2, output_dict=True))","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"           contradiction     entailment        neutral  accuracy  \\\nprecision       0.729953       0.701692       0.744365  0.724097   \nrecall          0.711863       0.767398       0.692905  0.724097   \nf1-score        0.720795       0.733075       0.717713  0.724097   \nsupport    183187.000000  183416.000000  182764.000000  0.724097   \n\n               macro avg   weighted avg  \nprecision       0.725337       0.725312  \nrecall          0.724055       0.724097  \nf1-score        0.723861       0.723870  \nsupport    549367.000000  549367.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.729953</td>\n      <td>0.701692</td>\n      <td>0.744365</td>\n      <td>0.724097</td>\n      <td>0.725337</td>\n      <td>0.725312</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.711863</td>\n      <td>0.767398</td>\n      <td>0.692905</td>\n      <td>0.724097</td>\n      <td>0.724055</td>\n      <td>0.724097</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.720795</td>\n      <td>0.733075</td>\n      <td>0.717713</td>\n      <td>0.724097</td>\n      <td>0.723861</td>\n      <td>0.723870</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>183187.000000</td>\n      <td>183416.000000</td>\n      <td>182764.000000</td>\n      <td>0.724097</td>\n      <td>549367.000000</td>\n      <td>549367.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_dev[\"gold_label\"], test_preds_2, output_dict=True))","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"           contradiction   entailment      neutral  accuracy    macro avg  \\\nprecision       0.664024     0.673609     0.681060  0.672729     0.672898   \nrecall          0.664430     0.709222     0.643586  0.672729     0.672412   \nf1-score        0.664227     0.690957     0.661793  0.672729     0.672326   \nsupport      3278.000000  3329.000000  3235.000000  0.672729  9842.000000   \n\n           weighted avg  \nprecision      0.672866  \nrecall         0.672729  \nf1-score       0.672468  \nsupport     9842.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.664024</td>\n      <td>0.673609</td>\n      <td>0.681060</td>\n      <td>0.672729</td>\n      <td>0.672898</td>\n      <td>0.672866</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.664430</td>\n      <td>0.709222</td>\n      <td>0.643586</td>\n      <td>0.672729</td>\n      <td>0.672412</td>\n      <td>0.672729</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.664227</td>\n      <td>0.690957</td>\n      <td>0.661793</td>\n      <td>0.672729</td>\n      <td>0.672326</td>\n      <td>0.672468</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>3278.000000</td>\n      <td>3329.000000</td>\n      <td>3235.000000</td>\n      <td>0.672729</td>\n      <td>9842.000000</td>\n      <td>9842.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"By comparing the test accuracies obtained by using *sentence1* (0.6) and *sentence2* (0.33) for the small dataset, we see that basing the predictions on *sentence2* seems to result in better results. So, we would choose the model using *sentence2*."},{"metadata":{},"cell_type":"markdown","source":"## Problem 5: A classifier based on manually engineered features"},{"metadata":{},"cell_type":"markdown","source":"[Bowman et al., 2015](https://www.aclweb.org/anthology/D15-1075/) evaluate a classifier that uses (among others) **cross-unigram features**. This term is used to refer to pairs of unigrams $(w_1, w_2)$ such that $w_1$ occurs in sentence&nbsp;1, $w_2$ occurs in sentence&nbsp;2, and both have been assigned the same part-of-speech tag.\n\nYour next task is to implement the cross-unigram classifier. To this end, the next cell contains skeleton code for a transformer that you can use as the first component in a classification pipeline. This transformer converts each row of the SNLI data frame into a space-separated string consisting of\n\n* the standard unigrams (of sentence&nbsp;1 or sentence&nbsp;2 – this depends on your results in Problem&nbsp;4)\n* the cross-unigrams, as defined above.\n\nThe space-separated string forms a new &lsquo;document&rsquo; that can be passed to a vectorizer in exactly the same way as a standard sentence in Problem&nbsp;4."},{"metadata":{},"cell_type":"markdown","source":"We have selected to use the standard unigrams of *sentence2* based on the results for the previous question as it leads to better test accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"class CrossUnigramsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    # Vectorize a sentence-tag-sentence-tag quadruple.\n    def _transform(self, sentence1, tags1, sentence2, tags2):\n        # Tokenize sentence and tag (also removing stop words and non-alpha words and their tags)\n        with nlp.disable_pipes(\"tagger\", \"parser\", \"ner\"):\n            words_tags1 = [(w.text, t) for w, t in zip(nlp(sentence1), tags1.split()) if w.is_alpha and not w.is_stop]\n            words_tags2 = [(w.text, t) for w, t in zip(nlp(sentence2), tags2.split()) if w.is_alpha and not w.is_stop]\n        \n        # Filter out tags that do not match\n        cross_unigrams = [wt1[0] + \"_\" + wt2[0] for wt1,wt2 in \n                          itertools.product(words_tags1, words_tags2) if wt1[1]==wt2[1]]\n        \n        # Combine standard unigrams and cross unigrams\n        return \" \".join([w for w, t in words_tags2] + cross_unigrams)\n\n    def transform(self, X):\n        return [self._transform(row[0], row[1], row[2], row[3]) for i,row in X.iterrows()]","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you have an implementation of the transformer, extend the pipeline that you built for Problem&nbsp;4, train it, and evaluate it on the development data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Enter code here to implement the cross-unigrams classifier. Print the classification report.\ncnt_cu_vectorizer = CountVectorizer(ngram_range=(1,1))\nlogreg_cu = LogisticRegression(solver=\"saga\", multi_class=\"ovr\", max_iter=50)\ncross_unigrams = CrossUnigramsTransformer()\n\nnp.random.seed(10)\nlogcnt_cu_pipeline = Pipeline([(\"cross_unigrams\", cross_unigrams), (\"cnt_vectorizer\", cnt_cu_vectorizer), \n                               (\"logreg\", logreg_cu)])\nlogcnt_cu_model = clone(logcnt_cu_pipeline).fit(df_train.loc[:, df_train.columns != \"gold_label\"], df_train[\"gold_label\"])","execution_count":39,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cu_preds, dev_cu_preds = get_predictions(logcnt_cu_model, [df_train.loc[:, df_train.columns != \"gold_label\"],\n                                                                 df_dev.loc[:, df_dev.columns != \"gold_label\"]])","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_train[\"gold_label\"], train_cu_preds, output_dict=True))","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"           contradiction     entailment        neutral  accuracy  \\\nprecision       0.841323       0.758726       0.826310  0.805116   \nrecall          0.806602       0.874051       0.734444  0.805116   \nf1-score        0.823597       0.812316       0.777674  0.805116   \nsupport    183187.000000  183416.000000  182764.000000  0.805116   \n\n               macro avg   weighted avg  \nprecision       0.808786       0.808752  \nrecall          0.805033       0.805116  \nf1-score        0.804529       0.804553  \nsupport    549367.000000  549367.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.841323</td>\n      <td>0.758726</td>\n      <td>0.826310</td>\n      <td>0.805116</td>\n      <td>0.808786</td>\n      <td>0.808752</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.806602</td>\n      <td>0.874051</td>\n      <td>0.734444</td>\n      <td>0.805116</td>\n      <td>0.805033</td>\n      <td>0.805116</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.823597</td>\n      <td>0.812316</td>\n      <td>0.777674</td>\n      <td>0.805116</td>\n      <td>0.804529</td>\n      <td>0.804553</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>183187.000000</td>\n      <td>183416.000000</td>\n      <td>182764.000000</td>\n      <td>0.805116</td>\n      <td>549367.000000</td>\n      <td>549367.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_dev[\"gold_label\"], dev_cu_preds, output_dict=True))","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"           contradiction   entailment      neutral  accuracy    macro avg  \\\nprecision       0.756434     0.700966     0.728688  0.726885     0.728696   \nrecall          0.735204     0.806248     0.636785  0.726885     0.726079   \nf1-score        0.745668     0.749930     0.679644  0.726885     0.725081   \nsupport      3278.000000  3329.000000  3235.000000  0.726885  9842.000000   \n\n           weighted avg  \nprecision      0.728552  \nrecall         0.726885  \nf1-score       0.725408  \nsupport     9842.000000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>contradiction</th>\n      <th>entailment</th>\n      <th>neutral</th>\n      <th>accuracy</th>\n      <th>macro avg</th>\n      <th>weighted avg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>precision</th>\n      <td>0.756434</td>\n      <td>0.700966</td>\n      <td>0.728688</td>\n      <td>0.726885</td>\n      <td>0.728696</td>\n      <td>0.728552</td>\n    </tr>\n    <tr>\n      <th>recall</th>\n      <td>0.735204</td>\n      <td>0.806248</td>\n      <td>0.636785</td>\n      <td>0.726885</td>\n      <td>0.726079</td>\n      <td>0.726885</td>\n    </tr>\n    <tr>\n      <th>f1-score</th>\n      <td>0.745668</td>\n      <td>0.749930</td>\n      <td>0.679644</td>\n      <td>0.726885</td>\n      <td>0.725081</td>\n      <td>0.725408</td>\n    </tr>\n    <tr>\n      <th>support</th>\n      <td>3278.000000</td>\n      <td>3329.000000</td>\n      <td>3235.000000</td>\n      <td>0.726885</td>\n      <td>9842.000000</td>\n      <td>9842.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"By using a CountVectorizer + LogisticRegression model and features obtained from sentence2 and cross-unigrams for the small dataset, we have obtained a test accuracy of 62.5% which is an improvement of approximately 2.2% compared to the previous model using only sentence2 features."},{"metadata":{},"cell_type":"markdown","source":"## Problem 6: A classifier based on word embeddings"},{"metadata":{},"cell_type":"markdown","source":"Your last task in this lab is to build a classifier for the natural language inference task that uses word embeddings. More specifically, we ask you to implement a vectorizer that represents each sentence as the sum of its word vectors – a representation known as the **continuous bag-of-words**. Thus, given that spaCy&rsquo;s word vectors have 300 dimensions, each sentence will be transformed into a 300-dimensional vector. To represent a sentence pair, the vectorizer should concatenate the vectors for the individual sentences; this yields a 600-dimensional vector. This vector can then be passed to a classifier.\n\nThe next code cell contains skeleton code for the vectorizer. You will have to implement two methods: one that maps a single sentence to a vector (of length 300), and one that maps a sentence pair to a vector (of length 600)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class PairedSentenceVectorizer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    # Vectorize a single sentence\n    def _transform1(self, sentence):\n        with nlp.disable_pipes(\"tagger\", \"parser\", \"ner\"):\n            return np.sum(np.array([w.vector for w in nlp(sentence)]), axis=0, keepdims=True)\n\n    # Vectorize a pair of sentences\n    def _transform2(self, sentence1, sentence2):\n        return np.hstack([self._transform1(sentence1), self._transform1(sentence2)])\n\n    def transform(self, X):\n        return np.concatenate([self._transform2(row[0], row[1]).reshape(1, -1) for i,row in X.iterrows()])","execution_count":43,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Once you have a working implementation, build a pipeline consisting of the new vectorizer and a [multi-layer perceptron classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html). This more powerful (compared to logistic regression) classifier is called for here because we do not specify features by hand (as we did in Problem&nbsp;5), but want to let the model learn a good representation of the data by itself. Use 3&nbsp;hidden layers, each with size 300. It suffices to train the classifier for 8&nbsp;iterations (epochs)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: Enter code here to implement the word embeddings classifier. Print the classification report.\nnp.random.seed(123)\npsent_vectorizer = PairedSentenceVectorizer()\nmlp_classifier = MLPClassifier(hidden_layer_sizes=(200, 100), activation=\"logistic\", early_stopping=True, validation_fraction=0.1,\n                               learning_rate=\"invscaling\", batch_size=32, verbose=3)\n\nmlp_pipeline = Pipeline([(\"psent_vectorizer\", psent_vectorizer), (\"mlp_classifier\", mlp_classifier)])\n# mlp_model = clone(mlp_pipeline).fit(df_train.loc[:, [\"sentence1\", \"sentence2\"]], df_train[\"gold_label\"])\nmlp_model = joblib.load('../input/db-text-mining-lab4/mlp_model_full_3x300.pkl')","execution_count":44,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../input/mlp_model_full_3x300.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-44-08196a5d6180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmlp_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"psent_vectorizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsent_vectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"mlp_classifier\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_classifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# mlp_model = clone(mlp_pipeline).fit(df_train.loc[:, [\"sentence1\", \"sentence2\"]], df_train[\"gold_label\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmlp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/mlp_model_full_3x300.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/mlp_model_full_3x300.pkl'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_mlp_preds, dev_mlp_preds = get_predictions(mlp_model, [df_train.loc[:, [\"sentence1\", \"sentence2\"]],\n                                                             df_dev.loc[:, [\"sentence1\", \"sentence2\"]] ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_train[\"gold_label\"], train_mlp_preds, output_dict=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(classification_report(df_dev[\"gold_label\"], dev_mlp_preds, output_dict=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final evaluation"},{"metadata":{},"cell_type":"markdown","source":"Once you have working code for all problems, re-run the code for Problems&nbsp;4–6 with the full training data. What are your results? How do they differ from the results that you obtained for the smaller training data? How do you interpret this? Summarize your findings in a short text."},{"metadata":{},"cell_type":"markdown","source":"**Problem 4:**\n\nDummy Classifier:    \nSmall dataset: Training accuracy = 0.33376, Test accuracy = 0.333774    \nFull dataset: Training accuracy = 0.333338, Test accuracy = 0.34495\n\n\nThe Dummy Classifier performs very similarly after training with both small and full dataset. The accuracy of approximately 0.33 makes sense as it matches the probability of guessing the class randomly when there are 3 classes.\n\n\nOne-sided baseline with sentence1 (Logistic Regression Classifier):    \nSmall dataset: Training accuracy = 0.33864, Test accuracy = 0.331335    \nFull dataset: Training accuracy = 0.338346, Test accuracy = 0.334688\n\n\nOne-sided baseline - sentence2 (Logistic Regression Classifier):    \nSmall dataset: Training accuracy = 0.745, Test accuracy = 0.602825    \nFull dataset: Training accuracy = 0.724097, Test accuracy = 0.672729\n\n\nIt looks like the one sided baseline using sentence 1 performs at the same level as a Dummy Classifier for both small and full dataset. This probably means that sentence 1 alone does not contain any features that can help in this classification task. However, we get a much better accuracy when we use sentence 2. The accuracy is further improved when we train it on the full dataset as compared to the small dataset. This probably means that sentence 2 on its own contains features which can help in this classification task. Also, more data leads to increase in accuracy. This is probably because the model would encounter more varied words and contexts and generalize better to the task. This is evidenced by how the training accuracy is lower but the test accuracy is much higher and the difference between training and test accuracies is also lower while using the full dataset for training.\n\n\n**Problem 5:**\n\nCross-unigrams + sentence2 Logistic Regression Classifier:    \nSmall dataset: Training accuracy = 0.87944, Test accuracy = 0.624975    \nFull dataset: Training accuracy = 0.805116, Test accuracy = 0.726885\n\n\nBy adding cross-unigrams, we observe that we are able to improve the accuracy further as compared to using only sentence 2. Also, using the full dataset leads to almost an increase of 0.1 in accuracy compared to the small dataset. This probably means that the cross unigrams are able to add more features which help in this classification task. It makes sense that considering words from both sentences leads to better accuracy because that would be able to represent the relationship between the sentences better. This model seems to be generalizing better when we use the full dataset. This is evidenced by how the training accuracy is lower but the test accuracy is much higher and the difference between training and test accuracies is also lower while using the full dataset for training.\n\n\n**Problem 6:**\n\nMLPClassifier with hidden units = (200, 100):    \nSmall dataset: Training accuracy = 0.73712, Test accuracy = 0.643873    \nFull dataset: Training accuracy = 0.769373, Test accuracy = 0.742532\n\n\nWe get a further 2% increase in test accuracy by using both small and full datasets. The model seems to be generalizing better than before for the small dataset also. The full dataset does lead to even better accuracy and generalization. This makes sense because we are able to encode the meaning of the sentence to a higher degree by using the word vectors. By concatenating the word vectors of the 2 sentences as a feature vector, we are able to compare the meanings of the 2 sentences at a more semantic level. Overall, the MLPClassifier model provides the best test accuracy of around 74%. \n"},{"metadata":{},"cell_type":"markdown","source":"<div class=\"alert alert-info\">\n    Please read the section ‘General information’ on the ‘Labs’ page of the course website before submitting this notebook!\n</div>"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}