{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a773bb63464f8ec67588cf9d6517d8faa72958d9"
   },
   "source": [
    "## Training a whole network\n",
    "So far, we have been training a single layer and we have been following the progress with the help of the Eager mode. If we want to train a whole network, it is much better to leave the Eager mode and to compile the graph. For this, we need to kill the notebook kernel (\"Kernel\" -> \"Restart\"). After that, we continue below in the classical TensorFlow mode.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c971a1ce0299a481651e05ab8bd7bf09bb58ecba"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version '+tf.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ae3bcba9660e3cfc3a065959e15b8d98715a8fe"
   },
   "source": [
    "**Task 12:** We will train on CIFAR10, which is contained in `keras.datasets`. Download the dataset, show a sample for each class, convert the labels to one-hot coding, and normalize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5c003999f96bf47c81a43831ee0106b0df87b5e"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "615a3ae670b5ce66b4ef05a387e02ee68e0f666e"
   },
   "outputs": [],
   "source": [
    "# Sample from each class\n",
    "\n",
    "classes = range(10)\n",
    "i = 0\n",
    "while classes:\n",
    "    if Y_train[i][0] in classes:\n",
    "        visualize(X_train[i])\n",
    "        classes = [x for x in classes if x!=Y_train[i][0]]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4270f6491a341fc06cd726df4bbf784f699ecff"
   },
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "\n",
    "objects_X = [X_train, X_test]\n",
    "normalized_objects_X = [[], []]\n",
    "for j in range(len(objects_X)):\n",
    "    obj = objects_X[j]\n",
    "    for i in range(len(obj)):\n",
    "            summed_img = np.sum(obj[i], axis = 2)\n",
    "            norm_image = np.expand_dims(summed_img / (255*3), 2)\n",
    "            #norm_image = np.expand_dims(summed_img / (255*3), 0)\n",
    "            normalized_objects_X[j].append(norm_image)\n",
    "normalized_X_train = np.array(normalized_objects_X[0])\n",
    "normalized_X_test = np.array(normalized_objects_X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22ae580ad3165ca31d83937fa006493dfcc5d0ac"
   },
   "outputs": [],
   "source": [
    "normalized_X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f19859f595d615f8f58c4290cbb2c47521c63118"
   },
   "outputs": [],
   "source": [
    "# Hot encoding\n",
    "\n",
    "hot_Y_train = tf.one_hot(Y_train, len(set(tuple(x) for x in Y_train)))\n",
    "hot_Y_test = tf.one_hot(Y_test, len(set(tuple(x) for x in Y_test)))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers\n",
    "encoder_train = LabelEncoder()\n",
    "encoder_train.fit(Y_train)\n",
    "encoded_Y_train = encoder_train.transform(Y_train)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_Y_train = np_utils.to_categorical(encoded_Y_train)\n",
    "\n",
    "# encode class values as integers\n",
    "encoder_test = LabelEncoder()\n",
    "encoder_test.fit(Y_test)\n",
    "encoded_Y_test = encoder_test.transform(Y_test)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_Y_test = np_utils.to_categorical(encoded_Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b06669f2d1672239268792fdd00feb912fbd494b"
   },
   "source": [
    "**Task 13:** Build the depicted LeNet5-inspired model using Keras standard components.\n",
    "![title](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa7da7986da9fb7df9848668c678ff7ecbcdec88"
   },
   "source": [
    "We will now train the network. For displaying the performance we define a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fef7544861d49b011603d9e51bd0fb2faa3c340b"
   },
   "outputs": [],
   "source": [
    "def plot_model_history(model):\n",
    "    line_w = 3\n",
    "    fig_size = (8, 6)\n",
    "    plt.figure(0)\n",
    "    plt.plot(model.history['acc'],'r', lw=line_w)\n",
    "    plt.plot(model.history['val_acc'],'b', lw=line_w)\n",
    "    plt.rcParams['figure.figsize'] = fig_size\n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "    plt.legend(['Training','Validation'])\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aec42c066c1102abdf95b5b147c4cd6c029c22a9"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "from keras.layers import Activation\n",
    "from keras import regularizers\n",
    "\n",
    "def base_model(x_train, num_classes):\n",
    "    model = Sequential()\n",
    "    print(x_train.shape[1:])\n",
    "    print(x_train.shape)\n",
    "    model.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3,3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    ###################################################################################\n",
    "    model.add(Conv2D(64, (3,3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    ###################################################################################\n",
    "    model.add(Conv2D(128, (3,3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding='same'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    ##################################################################################\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    opt = Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c7d97e6acb51203d996c286c2a9a5df7f6f3074"
   },
   "source": [
    "**Task 14:** Train the defined model for 50 epochs and a suitable batch size and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8d03628a6b148ab43f6bdc58700ce34c6a2995b"
   },
   "outputs": [],
   "source": [
    "out_trained_LeNet5 = base_model(normalized_X_train, 10).fit(\n",
    "    normalized_X_train, dummy_Y_train, batch_size = 250, epochs = 150, \n",
    "    validation_data = (normalized_X_test, dummy_Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7943dbb0055b9777f15e0d06bb8a0c890db3ea0a"
   },
   "outputs": [],
   "source": [
    "plot_model_history(out_trained_LeNet5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b37669f5c2b86a73fbff7d115038dd47adc4802"
   },
   "source": [
    "The results can be improved in several ways, e.g., by successively reducing the learning rate (divided by two after 30 and 40 epochs), by data augmentation (`ImageDataGenerator`), and by weight regularization (see above).  \n",
    "**Extra task:** Try to improve the result using one or several of the mentioned approaches.  \n",
    "Hint: you need to define a suitable callback function to change the learning rate during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
